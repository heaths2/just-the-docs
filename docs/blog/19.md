---
layout: default
title: 엔비디아 GPU 에러 확인방법
parent: Blog
grand_parent: Category
permalink: docs/category/blog/19
child_nav_order: desc
---

개요

> - 엔비디아 GPU 에러 확인방법이다.
> - [Useful nvidia-smi Queries](https://nvidia.custhelp.com/app/answers/detail/a_id/3751/~/useful-nvidia-smi-queries) 참조
> - [XID-Errors](https://docs.nvidia.com/deploy/xid-errors/index.html) 참조
{: .new }

NVIDIA GPU 에러 확인

```bash
nvidia-smi 
```

![nvidia-smi err](https://user-images.githubusercontent.com/36792594/153114156-ba6d670d-b4be-4f87-a63f-3d3b9a092349.png)

syslog 확인

```bash
grep -Eni "NVRM: Xid \(PCI:0000:1c:00\): 79" /var/log/syslog
```

```bash
10277:Feb  7 22:07:46 GPUSVR01 kernel: [  406.959833] NVRM: Xid (PCI:0000:1c:00): 79, pid=2109, GPU has fallen off the bus.
25637:Feb  8 10:00:28 GPUSVR01 kernel: [  472.044559] NVRM: Xid (PCI:0000:1c:00): 79, pid=6955, GPU has fallen off the bus.
```

```bash
Feb  7 22:07:45 GPUSVR01 kernel: [  405.906874] NVRM: Xid (PCI:0000:1c:00): 45, pid=1664, Ch 00000000
Feb  7 22:07:45 GPUSVR01 kernel: [  406.353306] sched: RT throttling activated
Feb  7 22:07:45 GPUSVR01 kernel: [  406.658203] NVRM: Xid (PCI:0000:1c:00): 45, pid=1664, Ch 00000001
Feb  7 22:07:45 GPUSVR01 kernel: [  406.863149] NVRM: Xid (PCI:0000:1c:00): 45, pid=3369, Ch 00000008
Feb  7 22:07:46 GPUSVR01 kernel: [  406.959833] NVRM: Xid (PCI:0000:1c:00): 79, pid=2109, GPU has fallen off the bus.
Feb  7 22:07:46 GPUSVR01 kernel: [  406.959835] NVRM: GPU 0000:1c:00.0: GPU has fallen off the bus.
Feb  7 22:07:46 GPUSVR01 kernel: [  406.959954] NVRM: GPU 0000:1c:00.0: GPU serial number is =>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\]^_`abcdefghijklmnopqrstuvwxyz{|}~��������������������.
Feb  7 22:07:46 GPUSVR01 kernel: [  406.961565] NVRM: A GPU crash dump has been created. If possible, please run
Feb  7 22:07:46 GPUSVR01 kernel: [  406.961565] NVRM: nvidia-bug-report.sh as root to collect this data before
Feb  7 22:07:46 GPUSVR01 kernel: [  406.961565] NVRM: the NVIDIA kernel module is unloaded.
Feb  7 22:07:46 GPUSVR01 kernel: [  406.961867] nvidia-modeset: ERROR: GPU:1: Failed to query display engine channel state: 0x0000c67d:0:0:0x0000000f
```

NVIDIA Bus Id 및 Serial Number 확인

```bash
vi /var/log/syslog
nvidia-smi  -q | grep -Ei "Bus Id|Serial Number"
```

```bash
    Serial Number                         : 1322721007180
        Bus Id                            : 00000000:01:00.0
    Serial Number                         : 1323821015392
        Bus Id                            : 00000000:1c:00.0
```

dmidecode PICE Slot 확인

```bash
dmidecode -t 9 | grep -Ei "Designation|Current Usage|Bus Address"
```

```bash
        Designation: PCIEX1_1
        Current Usage: Available
        Bus Address: 0000:ff:01.0
        Designation: PCIEX16_1
        Current Usage: In Use
        Bus Address: 0000:01:1c.3
        Designation: PCIEX1_2
        Current Usage: Available
        Bus Address: 0000:32:1c.4
        Designation: PCI1
        Current Usage: Available
        Bus Address: 0000:34:1c.6
        Designation: PCIEX16_2
        Current Usage: In Use
        Bus Address: 0000:1c:00.0
        Designation: PCI2
        Current Usage: Available
        Bus Address: 0000:34:01.0
        Designation: PCIEX16_3
        Current Usage: Available
        Bus Address: 0000:03:1c.0
```

```bash
nvidia-smi --query-gpu=gpu_name,gpu_bus_id,vbios_version --format=csv
```

```bash
name, pci.bus_id, vbios_version
NVIDIA RTX A5000, 00000000:01:00.0, 94.02.6D.00.0D
NVIDIA RTX A5000, 00000000:02:00.0, 94.02.6D.00.0D
```

```bash
nvidia-smi --query-gpu=gpu_name,gpu_bus_id,vbios_version --format=csv -l 5
```

```bash
name, pci.bus_id, vbios_version
NVIDIA RTX A5000, 00000000:01:00.0, 94.02.6D.00.0D
NVIDIA RTX A5000, 00000000:02:00.0, 94.02.6D.00.0D
NVIDIA RTX A5000, 00000000:01:00.0, 94.02.6D.00.0D
NVIDIA RTX A5000, 00000000:02:00.0, 94.02.6D.00.0D
```

```bash
nvidia-smi --query-gpu=timestamp,name,pci.bus_id,driver_version,pstate,pcie.link.gen.max, pcie.link.gen.current,temperature.gpu,utilization.gpu,utilization.memory, memory.total,memory.free,memory.used --format=csv
```

```bash
timestamp, name, pci.bus_id, driver_version, pstate, pcie.link.gen.max
2022/02/09 06:19:00.700, NVIDIA RTX A5000, 00000000:01:00.0, 470.103.01, P8, 3
2022/02/09 06:19:00.701, NVIDIA RTX A5000, 00000000:02:00.0, 470.103.01, P8, 3
```

```bash
nvidia-smi --query-gpu=timestamp,name,pci.bus_id,driver_version,pstate,pcie.link.gen.max, pcie.link.gen.current,temperature.gpu,utilization.gpu,utilization.memory, memory.total,memory.free,memory.used --format=csv -l 5
```

```bash
timestamp, name, pci.bus_id, driver_version, pstate, pcie.link.gen.max
2022/02/09 06:18:34.630, NVIDIA RTX A5000, 00000000:01:00.0, 470.103.01, P8, 3
2022/02/09 06:18:34.630, NVIDIA RTX A5000, 00000000:02:00.0, 470.103.01, P8, 3
2022/02/09 06:18:39.631, NVIDIA RTX A5000, 00000000:01:00.0, 470.103.01, P8, 3
2022/02/09 06:18:39.631, NVIDIA RTX A5000, 00000000:02:00.0, 470.103.01, P8, 3
2022/02/09 06:18:44.631, NVIDIA RTX A5000, 00000000:01:00.0, 470.103.01, P8, 3
2022/02/09 06:18:44.632, NVIDIA RTX A5000, 00000000:02:00.0, 470.103.01, P8, 3
2022/02/09 06:18:49.632, NVIDIA RTX A5000, 00000000:01:00.0, 470.103.01, P8, 3
2022/02/09 06:18:49.632, NVIDIA RTX A5000, 00000000:02:00.0, 470.103.01, P8, 3
2022/02/09 06:18:54.633, NVIDIA RTX A5000, 00000000:01:00.0, 470.103.01, P8, 3
```
{: .important}